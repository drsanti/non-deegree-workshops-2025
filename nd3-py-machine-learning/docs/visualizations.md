# Visualization Documentation

This document describes all plots and visualizations generated by the Python scripts in this tutorial.

## Table of Contents

- [Chapter 1: Sensor Data Generation](#chapter-1-sensor-data-generation)
- [Chapter 2: Data Preprocessing](#chapter-2-data-preprocessing)
- [Chapter 3: Data Exploration](#chapter-3-data-exploration)
- [Chapter 4: Linear Regression](#chapter-4-linear-regression)
- [Chapter 5: Logistic Regression](#chapter-5-logistic-regression)
- [Chapter 6: Decision Trees](#chapter-6-decision-trees)
- [Chapter 7: Clustering and SVM](#chapter-7-clustering-and-svm)
- [Chapter 8: Neural Networks (Binary Classification)](#chapter-8-neural-networks-binary-classification)
- [Chapter 9: Neural Networks (Multi-class Classification)](#chapter-9-neural-networks-multi-class-classification)
- [Chapter 10: LSTM for Time-Series Fault Detection](#chapter-10-lstm-for-time-series-fault-detection)
- [CSV_TF Module Visualizations](#csv_tf-module-visualizations)

---

## Chapter 1: Sensor Data Generation

**Script**: `codes/chapter01.py`  
**Output File**: `output/chapter01-sensors.png`

### Multi-Sensor Time-Series Plot

A 2×2 subplot grid displaying four sensor readings over time:

1. **Temperature Sensor** (Top Left)
   - X-axis: Time (seconds)
   - Y-axis: Temperature (°C)
   - Color: Red
   - Shows temperature variation over time

2. **Vibration Sensor** (Top Right)
   - X-axis: Time (seconds)
   - Y-axis: Vibration (g - gravitational acceleration units)
   - Color: Blue
   - Shows vibration amplitude over time

3. **Pressure Sensor** (Bottom Left)
   - X-axis: Time (seconds)
   - Y-axis: Pressure (PSI)
   - Color: Green
   - Shows pressure readings over time

4. **Current Sensor** (Bottom Right)
   - X-axis: Time (seconds)
   - Y-axis: Current (A - Amperes)
   - Color: Orange
   - Shows electrical current over time

**Purpose**: Demonstrates how to generate and visualize synthetic sensor data for industrial monitoring applications.

---

## Chapter 2: Data Preprocessing

**Script**: `codes/chapter02.py`  
**Output File**: `output/chapter02-preprocessing.png`

### Preprocessing Techniques Visualization

A 2×2 subplot grid showing different preprocessing techniques applied to temperature data:

1. **Original vs Smoothed Temperature** (Top Left)
   - Original data (light, semi-transparent)
   - Smoothed data using moving average (bold line)
   - Demonstrates noise reduction through smoothing

2. **Temperature Gradient** (Top Right)
   - Shows the rate of change (first derivative) of temperature
   - Useful for detecting sudden changes or trends
   - Helps identify rapid temperature increases/decreases

3. **Rolling Standard Deviation** (Bottom Left)
   - Original temperature (light, semi-transparent)
   - Rolling standard deviation overlay (bold line)
   - Shows variability and volatility in the data
   - Useful for anomaly detection

4. **Normalization Comparison** (Bottom Right)
   - Min-Max Scaling (0-1 range)
   - Standard Scaling (mean=0, std=1)
   - Compares two common normalization techniques
   - Shows how different scaling affects data distribution

**Purpose**: Illustrates essential data preprocessing techniques: smoothing, gradient calculation, rolling statistics, and normalization.

---

## Chapter 3: Data Exploration

**Script**: `codes/chapter03.py`  
**Output Files**: 
- `output/chapter03-correlation.png`
- `output/chapter03-distributions.png`
- `output/chapter03-pairplot.png`
- `output/chapter03-boxplots.png`
- `output/chapter03-timeseries.png`

### 1. Correlation Matrix Heatmap

**File**: `output/chapter03-correlation.png`

- **Type**: Heatmap showing correlation coefficients between sensors
- **Sensors**: Temperature, Vibration, Pressure, Current
- **Color Scale**: Cool-warm colormap (blue = negative, red = positive)
- **Values**: Correlation coefficients annotated in each cell (-1 to +1)
- **Purpose**: Identifies relationships and dependencies between different sensor readings

**Interpretation**:
- Values close to +1: Strong positive correlation
- Values close to -1: Strong negative correlation
- Values close to 0: Weak or no correlation

### 2. Distribution Histograms

**File**: `output/chapter03-distributions.png`

A 2×2 subplot grid showing the distribution of each sensor:

1. **Temperature Distribution** (Top Left)
   - Histogram with 30 bins
   - Red color
   - Mean value marked with dashed line

2. **Vibration Distribution** (Top Right)
   - Histogram with 30 bins
   - Blue color
   - Mean value marked with dashed line

3. **Pressure Distribution** (Bottom Left)
   - Histogram with 30 bins
   - Green color
   - Mean value marked with dashed line

4. **Current Distribution** (Bottom Right)
   - Histogram with 30 bins
   - Orange color
   - Mean value marked with dashed line

**Purpose**: Shows the statistical distribution of each sensor, helping identify normal operating ranges and potential outliers.

### 3. Pairwise Scatter Plot Matrix

**File**: `output/chapter03-pairplot.png`

- **Type**: Pairplot (pairwise scatter plots with KDE on diagonal)
- **Sensors**: All four sensors (Temperature, Vibration, Pressure, Current)
- **Diagonal**: Kernel Density Estimation (KDE) plots showing univariate distributions
- **Off-Diagonal**: Scatter plots showing bivariate relationships
- **Sample Size**: 500 points (sampled for clarity)
- **Purpose**: Comprehensive view of relationships between all sensor pairs

**Interpretation**:
- Diagonal plots: Distribution shape for each sensor
- Scatter plots: Linear/non-linear relationships between sensors
- Helps identify feature interactions for machine learning

### 4. Box Plots

**File**: `output/chapter03-boxplots.png`

- **Type**: Box plot for outlier detection
- **Sensors**: Temperature, Vibration, Pressure, Current
- **Y-axis**: Normalized values
- **Purpose**: Identifies outliers, quartiles, and median values

**Box Plot Elements**:
- Box: Interquartile range (IQR)
- Line in box: Median
- Whiskers: 1.5×IQR range
- Points beyond whiskers: Outliers

### 5. Normalized Time-Series Comparison

**File**: `output/chapter03-timeseries.png`

- **Type**: Overlaid line plots
- **Sensors**: All four sensors normalized to 0-1 range
- **X-axis**: Time (seconds)
- **Y-axis**: Normalized value (0-1)
- **Purpose**: Compare temporal patterns across sensors on the same scale

**Interpretation**:
- Shows synchronized patterns across sensors
- Helps identify which sensors respond together to events
- Useful for detecting anomalies that affect multiple sensors

---

## Chapter 4: Linear Regression

**Script**: `codes/chapter04.py`  
**Output Files**:
- `output/chapter04-regression.png`
- `output/chapter04-prediction.png`

### 1. Regression Analysis Plot

**File**: `output/chapter04-regression.png`

A 1×2 subplot grid:

1. **Actual vs Predicted** (Left)
   - Scatter plot: Actual values (x-axis) vs Predicted values (y-axis)
   - Ideal line: y=x (perfect prediction)
   - Shows model accuracy and prediction quality

2. **Residuals Plot** (Right)
   - Residuals (errors) vs Predicted values
   - Horizontal line at y=0 (no error)
   - Shows prediction errors distribution
   - Helps identify heteroscedasticity or patterns in errors

**Purpose**: Evaluates linear regression model performance and identifies prediction errors.

### 2. Future Prediction Plot

**File**: `output/chapter04-prediction.png`

- **Type**: Time-series plot with actual and predicted values
- **X-axis**: Time (extended into future)
- **Y-axis**: Temperature (°C)
- **Lines**:
  - Actual values (solid line)
  - Predicted values (dashed line)
- **Purpose**: Demonstrates model's ability to predict future values

**Interpretation**:
- Close overlap: Good prediction accuracy
- Divergence: Model may need improvement or more training data

---

## Chapter 5: Logistic Regression

**Script**: `codes/chapter05.py`  
**Output Files**:
- `output/chapter05-decision-boundary.png`
- `output/chapter05-probability.png`

### 1. Decision Boundary Plot

**File**: `output/chapter05-decision-boundary.png`

- **Type**: 2D scatter plot with decision boundary
- **Axes**: Two feature dimensions
- **Points**: 
  - Normal class (one color)
  - Fault class (another color)
- **Boundary**: Curved line separating the two classes
- **Background**: Colored regions showing class probabilities
- **Purpose**: Visualizes how the model separates classes in feature space

**Interpretation**:
- Points on correct side of boundary: Correctly classified
- Points on wrong side: Misclassified
- Boundary shape: Linear vs non-linear separation

### 2. Probability Distribution Plot

**File**: `output/chapter05-probability.png`

- **Type**: Histogram showing predicted probabilities
- **X-axis**: Predicted probability of fault (0 to 1)
- **Y-axis**: Frequency (number of samples)
- **Classes**: 
  - Normal samples (one color)
  - Fault samples (another color)
- **Threshold**: Vertical dashed line at 0.5
- **Purpose**: Shows model's confidence in predictions

**Interpretation**:
- Well-separated distributions: Good model performance
- Overlapping distributions: Model uncertainty or need for improvement
- Threshold position: Decision boundary for binary classification

---

## Chapter 6: Decision Trees

**Script**: `codes/chapter06.py`  
**Output Files**:
- `output/chapter06-feature-importance.png`
- `output/chapter06-confusion-matrices.png`

### 1. Feature Importance Plot

**File**: `output/chapter06-feature-importance.png`

A 1×2 subplot grid:

1. **Feature Importance (Decision Tree)** (Left)
   - Horizontal bar chart
   - Shows which features are most important for classification
   - Higher bars = more important features

2. **Feature Importance (Random Forest)** (Right)
   - Horizontal bar chart
   - Average importance across multiple trees
   - More stable than single tree importance

**Purpose**: Identifies which sensor readings are most critical for fault detection.

### 2. Confusion Matrices Comparison

**File**: `output/chapter06-confusion-matrices.png`

A 1×2 subplot grid:

1. **Decision Tree Confusion Matrix** (Left)
   - 2×2 heatmap
   - Classes: Normal, Fault
   - Shows classification performance

2. **Random Forest Confusion Matrix** (Right)
   - 2×2 heatmap
   - Classes: Normal, Fault
   - Shows improved performance with ensemble method

**Purpose**: Compares single decision tree vs ensemble (Random Forest) performance.

**Confusion Matrix Interpretation**:
- Diagonal (top-left to bottom-right): Correct predictions
- Off-diagonal: Misclassifications
- Darker colors: Higher counts

---

## Chapter 7: Clustering and SVM

**Script**: `codes/chapter07.py`  
**Output Files**:
- `output/chapter07-kmeans.png`
- `output/chapter07-svm.png`
- `output/chapter07-comparison.png`

### 1. K-Means Clustering Plot

**File**: `output/chapter07-kmeans.png`

- **Type**: 2D scatter plot with cluster centers
- **Points**: Colored by cluster assignment
- **Centers**: Marked with X symbols
- **Clusters**: Typically 2-4 clusters
- **Purpose**: Shows how K-means groups similar data points

**Interpretation**:
- Points of same color: Similar characteristics
- Cluster centers: Representative of each group
- Separation: How distinct the clusters are

### 2. SVM Decision Boundary Plot

**File**: `output/chapter07-svm.png`

- **Type**: 2D scatter plot with SVM decision boundary
- **Points**: 
  - Normal class (one color)
  - Fault class (another color)
- **Boundary**: Line/curve separating classes
- **Support Vectors**: Marked points (if shown)
- **Margin**: Distance between boundary and support vectors
- **Purpose**: Visualizes SVM's classification approach

**Interpretation**:
- Wide margin: More robust classification
- Support vectors: Critical data points defining the boundary

### 3. Clustering vs Classification Comparison

**File**: `output/chapter07-comparison.png`

A 1×2 subplot grid:

1. **K-Means Clustering** (Left)
   - Unsupervised learning result
   - Groups data without labels

2. **SVM Classification** (Right)
   - Supervised learning result
   - Uses labels for training

**Purpose**: Compares unsupervised (clustering) vs supervised (classification) approaches.

---

## Chapter 8: Neural Networks (Binary Classification)

**Script**: `codes/chapter08.py`  
**Output Files**:
- `output/chapter08-training-history.png`
- `output/chapter08-probability.png`
- `output/chapter08-confusion-matrix.png`
- `output/chapter08-feature-importance.png`

### 1. Training History Plot

**File**: `output/chapter08-training-history.png`

A 1×2 subplot grid:

1. **Model Loss** (Left)
   - Training loss (solid line)
   - Validation loss (dashed line)
   - X-axis: Epoch number
   - Y-axis: Loss value
   - Shows learning progress

2. **Model Accuracy** (Right)
   - Training accuracy (solid line)
   - Validation accuracy (dashed line)
   - X-axis: Epoch number
   - Y-axis: Accuracy (0-1)
   - Shows performance improvement

**Purpose**: Monitors training progress and detects overfitting.

**Interpretation**:
- Decreasing loss: Model is learning
- Increasing accuracy: Performance improving
- Large gap between train/val: Possible overfitting

### 2. Probability Distribution Plot

**File**: `output/chapter08-probability.png`

- **Type**: Histogram showing predicted probabilities
- **X-axis**: Predicted probability of fault (0 to 1)
- **Y-axis**: Frequency
- **Classes**: 
  - Normal samples (blue)
  - Fault samples (red)
- **Threshold**: Vertical dashed line at 0.5
- **Purpose**: Shows model confidence in binary classification

### 3. Confusion Matrix

**File**: `output/chapter08-confusion-matrix.png`

- **Type**: 2×2 heatmap
- **Classes**: Normal, Fault
- **Rows**: True labels
- **Columns**: Predicted labels
- **Color**: Blue scale (darker = higher count)
- **Purpose**: Shows classification performance metrics

**Metrics from Matrix**:
- True Positives (TP): Bottom-right
- True Negatives (TN): Top-left
- False Positives (FP): Top-right
- False Negatives (FN): Bottom-left

### 4. Feature Importance Plot

**File**: `output/chapter08-feature-importance.png`

- **Type**: Horizontal bar chart with error bars
- **Method**: Permutation importance
- **Features**: Feature 1, Feature 2, Feature 3, Feature 4
- **Error Bars**: Standard deviation across multiple permutations
- **Purpose**: Identifies which input features are most important for predictions

**Interpretation**:
- Longer bars: More important features
- Error bars: Stability of importance measurement

---

## Chapter 9: Neural Networks (Multi-class Classification)

**Script**: `codes/chapter09.py`  
**Output Files**:
- `output/chapter09-training-history.png`
- `output/chapter09-confusion-matrix.png`
- `output/chapter09-probabilities.png`
- `output/chapter09-per-class-accuracy.png`

### 1. Training History Plot

**File**: `output/chapter09-training-history.png`

A 1×2 subplot grid:

1. **Model Loss** (Left)
   - Training loss vs Validation loss
   - Shows learning convergence

2. **Model Accuracy** (Right)
   - Training accuracy vs Validation accuracy
   - Shows performance improvement

**Purpose**: Monitors multi-class classification training progress.

### 2. Confusion Matrix

**File**: `output/chapter09-confusion-matrix.png`

- **Type**: N×N heatmap (where N = number of classes)
- **Classes**: Multiple fault conditions (e.g., Normal, Bearing, Misalignment, Imbalance)
- **Purpose**: Shows classification performance for each class

**Interpretation**:
- Diagonal: Correct predictions per class
- Off-diagonal: Misclassifications between classes

### 3. Probability Distributions

**File**: `output/chapter09-probabilities.png`

A 2×2 subplot grid showing probability distributions for each class:

- **Per-class histograms**: Predicted probabilities for each class
- **X-axis**: Probability (0-1)
- **Y-axis**: Frequency
- **Purpose**: Shows model confidence for each class

**Interpretation**:
- High probabilities: Confident predictions
- Low probabilities: Uncertain predictions

### 4. Per-Class Accuracy Plot

**File**: `output/chapter09-per-class-accuracy.png`

- **Type**: Bar chart
- **X-axis**: Class names
- **Y-axis**: Accuracy (0-1)
- **Purpose**: Shows which classes are easier/harder to classify

**Interpretation**:
- Higher bars: Better classification for that class
- Lower bars: Classes that need more training data or feature engineering

---

## Chapter 10: LSTM for Time-Series Fault Detection

**Script**: `codes/chapter10.py`  
**Output Files**:
- `output/chapter10-training-history.png`
- `output/chapter10-sequences.png`
- `output/chapter10-probability.png`
- `output/chapter10-confusion-matrix.png`
- `output/chapter10-fault-progression.png`

### 1. Training History Plot

**File**: `output/chapter10-training-history.png`

A 1×2 subplot grid:

1. **Model Loss** (Left)
   - Training and validation loss over epochs

2. **Model Accuracy** (Right)
   - Training and validation accuracy over epochs

**Purpose**: Monitors LSTM training for time-series classification.

### 2. Input Sequences Visualization

**File**: `output/chapter10-sequences.png`

A 2×2 subplot grid showing sample input sequences:

- **Time-series plots**: Example sequences for each class
- **X-axis**: Time steps
- **Y-axis**: Sensor values
- **Purpose**: Visualizes the temporal patterns the LSTM learns

**Interpretation**:
- Different patterns per class: Model can distinguish conditions
- Similar patterns: May cause confusion

### 3. Probability Distribution Plot

**File**: `output/chapter10-probability.png`

- **Type**: Histogram
- **X-axis**: Predicted probability
- **Y-axis**: Frequency
- **Classes**: Normal vs Fault
- **Purpose**: Shows LSTM's confidence in predictions

### 4. Confusion Matrix

**File**: `output/chapter10-confusion-matrix.png`

- **Type**: 2×2 heatmap
- **Classes**: Normal, Fault
- **Purpose**: LSTM classification performance

### 5. Fault Progression Over Time

**File**: `output/chapter10-fault-progression.png`

- **Type**: Time-series plot
- **X-axis**: Time (extended sequence)
- **Y-axis**: Fault probability (0-1)
- **Line**: Red line showing fault probability over time
- **Purpose**: Demonstrates how fault probability changes over time

**Interpretation**:
- Low values (<0.5): Normal operation
- High values (>0.5): Fault detected
- Gradual increase: Progressive fault development
- Sudden spike: Immediate fault occurrence

---

## CSV_TF Module Visualizations

The `csv_tf/` module contains additional visualizations for motor vibration data analysis. See [CSV_TF Visualization Documentation](../csv_tf/docs/visualizations.md) for detailed information.

### Quick Reference

**Training Scripts**:
- `train_lstm.py`: LSTM training history and confusion matrix
- `train_features.py`: Feature-based model training history and confusion matrix

**Testing Scripts**:
- `test_lstm.py`: LSTM test confusion matrix and probability distributions
- `test_features.py`: Feature-based test confusion matrix and probability distributions

**Validation Scripts**:
- `validate.py`: Sample visualizations and condition comparisons

---

## General Plot Interpretation Guidelines

### Training History Plots
- **Good signs**: Decreasing loss, increasing accuracy, train/val curves close together
- **Warning signs**: Large gap between train/val (overfitting), loss not decreasing (underfitting)

### Confusion Matrices
- **Perfect**: All values on diagonal, zeros elsewhere
- **Good**: Most values on diagonal, few off-diagonal
- **Poor**: Many off-diagonal values (misclassifications)

### Probability Distributions
- **Well-separated**: Good model confidence
- **Overlapping**: Model uncertainty, may need improvement

### Feature Importance
- **High importance**: Critical features for predictions
- **Low importance**: May be redundant or noisy

---

## Output Directory Structure

All plots are saved in the `output/` directory:

```
output/
├── chapter01-sensors.png
├── chapter02-preprocessing.png
├── chapter03-correlation.png
├── chapter03-distributions.png
├── chapter03-pairplot.png
├── chapter03-boxplots.png
├── chapter03-timeseries.png
├── chapter04-regression.png
├── chapter04-prediction.png
├── chapter05-decision-boundary.png
├── chapter05-probability.png
├── chapter06-feature-importance.png
├── chapter06-confusion-matrices.png
├── chapter07-kmeans.png
├── chapter07-svm.png
├── chapter07-comparison.png
├── chapter08-training-history.png
├── chapter08-probability.png
├── chapter08-confusion-matrix.png
├── chapter08-feature-importance.png
├── chapter09-training-history.png
├── chapter09-confusion-matrix.png
├── chapter09-probabilities.png
├── chapter09-per-class-accuracy.png
├── chapter10-training-history.png
├── chapter10-sequences.png
├── chapter10-probability.png
├── chapter10-confusion-matrix.png
└── chapter10-fault-progression.png
```

---

## Notes

- All plots are saved at 150 DPI for high quality
- Plots are displayed in interactive windows using `plt.show()`
- Plots are saved using `plt.savefig()` before display
- Color schemes are consistent across related plots
- Grid lines and legends are included for clarity

